{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe3nk7FfObVE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import Any, List, Optional, Tuple, Dict\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i75syhQkOeRE"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voHZUIWXOD77"
      },
      "outputs": [],
      "source": [
        "def normalize_and_split(raw_text: str) -> List[str]:\n",
        "    cleaned = raw_text.lower()\n",
        "    cleaned = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", cleaned)\n",
        "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
        "    return cleaned.split()\n",
        "\n",
        "\n",
        "def read_poetry_file() -> str:\n",
        "    with open(\"poems.txt\", \"r\", encoding=\"utf-8\") as file_handle:\n",
        "        return file_handle.read()\n",
        "\n",
        "\n",
        "def create_vocabulary(token_stream: List[str],\n",
        "                      min_occurrence: int = 1\n",
        "                      ) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "\n",
        "    frequency_map = Counter(token_stream)\n",
        "\n",
        "    base_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "    vocab_list = list(base_tokens)\n",
        "\n",
        "    for token, freq in frequency_map.items():\n",
        "        if freq >= min_occurrence and token not in vocab_list:\n",
        "            vocab_list.append(token)\n",
        "\n",
        "    token_to_index = {\n",
        "        token: idx for idx, token in enumerate(vocab_list)\n",
        "    }\n",
        "\n",
        "    index_to_token = {\n",
        "        idx: token for token, idx in token_to_index.items()\n",
        "    }\n",
        "\n",
        "    return token_to_index, index_to_token\n",
        "\n",
        "\n",
        "def encode_tokens(token_stream: List[str],\n",
        "                  token_to_index: Dict[str, int]\n",
        "                  ) -> List[int]:\n",
        "\n",
        "    unknown_id = token_to_index[\"<unk>\"]\n",
        "\n",
        "    encoded = [\n",
        "        token_to_index.get(tok, unknown_id)\n",
        "        for tok in token_stream\n",
        "    ]\n",
        "\n",
        "    return encoded\n",
        "\n",
        "\n",
        "def build_sequence_pairs(id_stream: List[int],\n",
        "                         window: int\n",
        "                         ) -> Tuple[List[List[int]], List[List[int]]]:\n",
        "\n",
        "    input_sequences = []\n",
        "    target_sequences = []\n",
        "\n",
        "    upper_bound = len(id_stream) - window\n",
        "\n",
        "    for start in range(upper_bound):\n",
        "        src = id_stream[start:start + window]\n",
        "        tgt = id_stream[start + 1:start + window + 1]\n",
        "\n",
        "        input_sequences.append(src)\n",
        "        target_sequences.append(tgt)\n",
        "\n",
        "    return input_sequences, target_sequences\n",
        "\n",
        "\n",
        "def analyze_text_statistics(sample_text: str) -> Dict[str, Any]:\n",
        "\n",
        "    tokens = normalize_and_split(sample_text)\n",
        "\n",
        "    if not tokens:\n",
        "        return {\n",
        "            \"token_count\": 0,\n",
        "            \"unique_tokens\": 0,\n",
        "            \"unique_ratio\": 0.0,\n",
        "            \"repeat_2gram_ratio\": 0.0,\n",
        "            \"repeat_3gram_ratio\": 0.0,\n",
        "            \"top_tokens\": [],\n",
        "        }\n",
        "\n",
        "    token_freq = Counter(tokens)\n",
        "\n",
        "    total_tokens = len(tokens)\n",
        "    unique_count = len(token_freq)\n",
        "    diversity_ratio = unique_count / max(total_tokens, 1)\n",
        "\n",
        "    def repeated_ngram_fraction(n: int) -> float:\n",
        "        if len(tokens) < n:\n",
        "            return 0.0\n",
        "\n",
        "        ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
        "        gram_freq = Counter(ngrams)\n",
        "\n",
        "        repeats = sum(v - 1 for v in gram_freq.values() if v > 1)\n",
        "        return repeats / max(len(ngrams), 1)\n",
        "\n",
        "    return {\n",
        "        \"token_count\": int(total_tokens),\n",
        "        \"unique_tokens\": int(unique_count),\n",
        "        \"unique_ratio\": float(diversity_ratio),\n",
        "        \"repeat_2gram_ratio\": float(repeated_ngram_fraction(2)),\n",
        "        \"repeat_3gram_ratio\": float(repeated_ngram_fraction(3)),\n",
        "        \"top_tokens\": [\n",
        "            (word, int(count))\n",
        "            for word, count in token_freq.most_common(10)\n",
        "        ],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fca1697",
        "outputId": "fc18cc5e-70bb-47c0-8372-9dd724c5717e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | avg loss: 301.3461\n",
            "Sample: masts : in our my in our my in forgetting this in emma this in emma this in sea this may emma ' in when ' seen our this in\n",
            "Epoch time: 85.784s\n",
            "Epoch 2 | avg loss: 323.4304\n",
            "Sample: pains tis our emma and wild our emma are wild our emma are wild our emma are wild our emma are wild our emma are wild our emma are wild\n",
            "Epoch time: 87.461s\n",
            "Epoch 3 | avg loss: 367.3105\n",
            "Sample: days death ' wild our green and wild ; green and wild our green this wild our green this wild our green this wild our green and wild our emma\n",
            "Epoch time: 94.385s\n",
            "Epoch 4 | avg loss: 365.7155\n",
            "Sample: tea about our , and name of , and call of , seen myself of , and name of , and told are , and call to , seen name\n",
            "Epoch time: 88.021s\n",
            "Epoch 5 | avg loss: 366.0527\n",
            "Sample: high side , we they course had thee it have , we it have , we sky have , we it have , thee they , had name it have\n",
            "Epoch time: 96.653s\n",
            "Epoch 6 | avg loss: 364.6420\n",
            "Sample: lance name swift , seen wild they , seen wild they , seen wild they , breathe wild they , seen wild holding , may wild they , seen wild\n",
            "Epoch time: 80.329s\n",
            "Epoch 7 | avg loss: 362.9818\n",
            "Sample: might see this wild to , this wild our , this wild to , this wild our , this wild to of this wild to of this wild to ,\n",
            "Epoch time: 83.246s\n",
            "Epoch 8 | avg loss: 361.2134\n",
            "Sample: hub see it have this should to have this joins they have this gold it have this rest to have this and to have this that they have this whom\n",
            "Epoch time: 81.714s\n",
            "Epoch 9 | avg loss: 359.9116\n",
            "Sample: ll practis ' name our emma and name to emma ' call our emma and wild to my and wild to emma ' wild our piled and wild our could\n",
            "Epoch time: 77.029s\n",
            "Epoch 10 | avg loss: 358.7984\n",
            "Sample: agony dear our of and wild our green and wild our green and least our green and wild they green and wild our of and wild our green and wild\n",
            "Epoch time: 73.651s\n"
          ]
        }
      ],
      "source": [
        "def stable_softmax(vec):\n",
        "    shifted = vec - np.max(vec)\n",
        "    exp_vals = np.exp(shifted)\n",
        "    return exp_vals / np.sum(exp_vals)\n",
        "\n",
        "\n",
        "def vector_one_hot(index, vocab_dim):\n",
        "    basis = np.zeros((vocab_dim, 1))\n",
        "    basis[index] = 1.0\n",
        "    return basis\n",
        "\n",
        "\n",
        "class NumpyRNNCore:\n",
        "\n",
        "    def __init__(self, vocab_dim, hidden_dim=64,\n",
        "                 learning_rate=1e-2, rng_seed=42):\n",
        "\n",
        "        rng = np.random.default_rng(rng_seed)\n",
        "\n",
        "        self.vocab_dim = vocab_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        self.W_input = rng.normal(0, 0.01,\n",
        "                                  (hidden_dim, vocab_dim))\n",
        "        self.W_hidden = rng.normal(0, 0.01,\n",
        "                                   (hidden_dim, hidden_dim))\n",
        "        self.W_output = rng.normal(0, 0.01,\n",
        "                                   (vocab_dim, hidden_dim))\n",
        "\n",
        "        self.b_hidden = np.zeros((hidden_dim, 1))\n",
        "        self.b_output = np.zeros((vocab_dim, 1))\n",
        "\n",
        "    # ---------- forward pass ----------\n",
        "\n",
        "    def forward_pass(self, token_ids, h_prev):\n",
        "\n",
        "        x_cache, h_cache, prob_cache = {}, {}, {}\n",
        "        h_cache[-1] = h_prev\n",
        "\n",
        "        for step, tok_id in enumerate(token_ids):\n",
        "\n",
        "            x_cache[step] = vector_one_hot(\n",
        "                tok_id, self.vocab_dim\n",
        "            )\n",
        "\n",
        "            h_cache[step] = np.tanh(\n",
        "                self.W_input @ x_cache[step]\n",
        "                + self.W_hidden @ h_cache[step - 1]\n",
        "                + self.b_hidden\n",
        "            )\n",
        "\n",
        "            logits = self.W_output @ h_cache[step] + self.b_output\n",
        "            probs = stable_softmax(logits.ravel()).reshape(-1, 1)\n",
        "\n",
        "            prob_cache[step] = probs\n",
        "\n",
        "        return x_cache, h_cache, prob_cache\n",
        "\n",
        "    # ---------- loss + gradients ----------\n",
        "\n",
        "    def backward_pass(self, token_ids, targets, h_prev):\n",
        "\n",
        "        xs, hs, ps = self.forward_pass(token_ids, h_prev)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        for t in range(len(token_ids)):\n",
        "            total_loss += -np.log(\n",
        "                ps[t][targets[t], 0] + 1e-12\n",
        "            )\n",
        "\n",
        "        g_Wi = np.zeros_like(self.W_input)\n",
        "        g_Wh = np.zeros_like(self.W_hidden)\n",
        "        g_Wo = np.zeros_like(self.W_output)\n",
        "        g_bh = np.zeros_like(self.b_hidden)\n",
        "        g_bo = np.zeros_like(self.b_output)\n",
        "\n",
        "        dh_next = np.zeros((self.hidden_dim, 1))\n",
        "\n",
        "        for t in reversed(range(len(token_ids))):\n",
        "\n",
        "            grad_out = ps[t].copy()\n",
        "            grad_out[targets[t]] -= 1.0\n",
        "\n",
        "            g_Wo += grad_out @ hs[t].T\n",
        "            g_bo += grad_out\n",
        "\n",
        "            dh = self.W_output.T @ grad_out + dh_next\n",
        "            dh_raw = (1 - hs[t] * hs[t]) * dh\n",
        "\n",
        "            g_bh += dh_raw\n",
        "            g_Wi += dh_raw @ xs[t].T\n",
        "            g_Wh += dh_raw @ hs[t - 1].T\n",
        "\n",
        "            dh_next = self.W_hidden.T @ dh_raw\n",
        "\n",
        "        for grad in [g_Wi, g_Wh, g_Wo, g_bh, g_bo]:\n",
        "            np.clip(grad, -5, 5, out=grad)\n",
        "\n",
        "        last_hidden = hs[len(token_ids) - 1]\n",
        "\n",
        "        return total_loss, (g_Wi, g_Wh, g_Wo, g_bh, g_bo), last_hidden\n",
        "\n",
        "    # ---------- parameter update ----------\n",
        "\n",
        "    def apply_gradients(self, grads):\n",
        "\n",
        "        g_Wi, g_Wh, g_Wo, g_bh, g_bo = grads\n",
        "\n",
        "        self.W_input -= self.lr * g_Wi\n",
        "        self.W_hidden -= self.lr * g_Wh\n",
        "        self.W_output -= self.lr * g_Wo\n",
        "        self.b_hidden -= self.lr * g_bh\n",
        "        self.b_output -= self.lr * g_bo\n",
        "\n",
        "    # ---------- sampling ----------\n",
        "\n",
        "    def generate_text(self, start_token, id_to_token,\n",
        "                      steps=30, temp=1.0):\n",
        "\n",
        "        hidden = np.zeros((self.hidden_dim, 1))\n",
        "        current = vector_one_hot(start_token,\n",
        "                                 self.vocab_dim)\n",
        "\n",
        "        output_tokens = []\n",
        "\n",
        "        for _ in range(steps):\n",
        "\n",
        "            hidden = np.tanh(\n",
        "                self.W_input @ current\n",
        "                + self.W_hidden @ hidden\n",
        "                + self.b_hidden\n",
        "            )\n",
        "\n",
        "            logits = self.W_output @ hidden + self.b_output\n",
        "            probs = stable_softmax(\n",
        "                logits.ravel() / max(temp, 1e-6)\n",
        "            )\n",
        "\n",
        "            sampled = np.random.choice(\n",
        "                range(self.vocab_dim),\n",
        "                p=probs\n",
        "            )\n",
        "\n",
        "            output_tokens.append(id_to_token[sampled])\n",
        "            current = vector_one_hot(sampled,\n",
        "                                     self.vocab_dim)\n",
        "\n",
        "        return \" \".join(output_tokens)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def scratch_rnn_notebook_run():\n",
        "\n",
        "    raw_text = read_poetry_file()\n",
        "    token_stream = (\n",
        "        [\"<bos>\"]\n",
        "        + normalize_and_split(raw_text)\n",
        "        + [\"<eos>\"]\n",
        "    )\n",
        "\n",
        "    token_to_id, id_to_token = create_vocabulary(\n",
        "        token_stream, min_occurrence=1\n",
        "    )\n",
        "\n",
        "    encoded_stream = encode_tokens(\n",
        "        token_stream, token_to_id\n",
        "    )\n",
        "\n",
        "    model = NumpyRNNCore(\n",
        "        vocab_dim=len(token_to_id),\n",
        "        hidden_dim=128,\n",
        "        learning_rate=0.05\n",
        "    )\n",
        "\n",
        "    window = 25\n",
        "    hidden_state = np.zeros((model.hidden_dim, 1))\n",
        "\n",
        "    timing_log = []\n",
        "    generated_samples = []\n",
        "\n",
        "    for epoch_idx in range(n_epochs):\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "        cumulative_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for pos in range(\n",
        "            0,\n",
        "            len(encoded_stream) - window - 1,\n",
        "            window\n",
        "        ):\n",
        "\n",
        "            inp = encoded_stream[pos:pos + window]\n",
        "            tgt = encoded_stream[pos + 1:pos + window + 1]\n",
        "\n",
        "            loss, grads, hidden_state = model.backward_pass(\n",
        "                inp, tgt, hidden_state\n",
        "            )\n",
        "\n",
        "            model.apply_gradients(grads)\n",
        "\n",
        "            cumulative_loss += loss\n",
        "            batch_count += 1\n",
        "\n",
        "        avg_loss = cumulative_loss / max(batch_count, 1)\n",
        "\n",
        "        sample_text = model.generate_text(\n",
        "            token_to_id[\"<bos>\"],\n",
        "            id_to_token,\n",
        "            steps=30,\n",
        "            temp=0.9\n",
        "        )\n",
        "\n",
        "        generated_samples.append(sample_text)\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        timing_log.append(float(end_time - start_time))\n",
        "\n",
        "        print(f\"Epoch {epoch_idx+1} | avg loss: {avg_loss:.4f}\")\n",
        "        print(\"Sample:\", sample_text)\n",
        "        print(f\"Epoch time: {timing_log[-1]:.3f}s\")\n",
        "\n",
        "    return timing_log, generated_samples[-1]\n",
        "\n",
        "\n",
        "scratch_epoch_times, scratch_last_output = scratch_rnn_notebook_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af8d8410",
        "outputId": "7fd75f3d-6134-4a95-9d08-412d91062287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training One-Hot RNN on mps\n",
            "Epoch 1 | loss: 6.2165\n",
            "Sample: <bos> lungs . long i rich be watches . borne with the lift of balanced , and or trouble , i who , house antique to ? about the pen and first . ? d that is bride it more dissatisfied\n",
            "Epoch time: 25.517s\n",
            "Epoch 2 | loss: 5.0680\n",
            "Sample: <bos> noon about -- ! so deny , their of circle following or rest the mine chemist when before emanations well , to give our white i ooze for the loves , you trusted i , i ' the little ,\n",
            "Epoch time: 23.657s\n",
            "Epoch 3 | loss: 3.9790\n",
            "Sample: <bos> are your object with her between us , who is not , now we thousand come trouble than others it is limitless to stronger upon ! retreat you die for ! i love , the sign ' d by the\n",
            "Epoch time: 23.614s\n",
            "Epoch 4 | loss: 2.9035\n",
            "Sample: <bos> to nothing , these thirty i know --great a knoll ago , no heart farewell no less part , and what is not ? never might ride . . \" \" o heart , i am friend powers ? )\n",
            "Epoch time: 23.741s\n",
            "Epoch 5 | loss: 1.8531\n",
            "Sample: <bos> of the end . there they are stopt life with a thousand life ' s work with lived church , as two , but see ever it is not how to so . till you would so man , when\n",
            "Epoch time: 24.012s\n",
            "Epoch 6 | loss: 1.0537\n",
            "Sample: <bos> , and look in the morning and barr half-held began at night . backward it led more lovely : and deeds ? the should be left up face all with them thoughts . how we will – with any thing\n",
            "Epoch time: 23.525s\n",
            "Epoch 7 | loss: 0.6251\n",
            "Sample: <bos> impel me . philip -- when bewildered bore his riddle in ! \" \" and cannot pleasures , while i gave that my name of summit i year . \" the little sparrows hop ingenuously about the pavement quarreling with\n",
            "Epoch time: 25.457s\n",
            "Epoch 8 | loss: 0.4415\n",
            "Sample: <bos> surrounding brighter and sounds through the day and new by him or the moccasin print , by the cot in the hospital reaching lemonade to a feverish patient , nigh the coffin ' d corpse when all is still ,\n",
            "Epoch time: 25.321s\n",
            "Epoch 9 | loss: 0.3659\n",
            "Sample: <bos> descry drives ? try and fare thee well the an earth ! and i follow stretch seek , of the side the bright green sound ; and those whose birds in too short together , where maim ' d and\n",
            "Epoch time: 25.054s\n",
            "Epoch 10 | loss: 0.3246\n",
            "Sample: <bos> fords the park ! you shall assume thus shall be their warm room : but ' d who comes to ! what the heaven never was an hour ; yet not ' d for things and yet , and hush\n",
            "Epoch time: 25.660s\n",
            "Total training time (one-hot): 245.56s\n"
          ]
        }
      ],
      "source": [
        "class OneHotSequenceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_sequences, target_sequences, vocab_dim):\n",
        "        self.inputs = torch.tensor(input_sequences, dtype=torch.long)\n",
        "        self.targets = torch.tensor(target_sequences, dtype=torch.long)\n",
        "        self.vocab_dim = vocab_dim\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.inputs.size(0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        token_ids = self.inputs[index]\n",
        "        label_ids = self.targets[index]\n",
        "\n",
        "        one_hot_tensor = torch.zeros(\n",
        "            token_ids.size(0),\n",
        "            self.vocab_dim,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        one_hot_tensor.scatter_(\n",
        "            1,\n",
        "            token_ids.unsqueeze(1),\n",
        "            1.0\n",
        "        )\n",
        "\n",
        "        return one_hot_tensor, label_ids\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "class OneHotLanguageRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.recurrent = nn.RNN(\n",
        "            input_size=vocab_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Linear(hidden_dim, vocab_dim)\n",
        "\n",
        "    def forward(self, x_encoded, h_state=None):\n",
        "\n",
        "        rnn_out, next_state = self.recurrent(x_encoded, h_state)\n",
        "        logits = self.projection(rnn_out)\n",
        "\n",
        "        return logits, next_state\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_sequence(model,\n",
        "                    token_to_id,\n",
        "                    id_to_token,\n",
        "                    seed=\"<bos>\",\n",
        "                    steps=40,\n",
        "                    temp=1.0):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    seed_tokens = seed.split()\n",
        "    id_stream = [\n",
        "        token_to_id.get(tok, token_to_id[\"<unk>\"])\n",
        "        for tok in seed_tokens\n",
        "    ]\n",
        "\n",
        "    vocab_dim = len(token_to_id)\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(steps):\n",
        "\n",
        "        last_token = torch.tensor(\n",
        "            id_stream[-1:],\n",
        "            dtype=torch.long,\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "        encoded = torch.zeros(\n",
        "            1, 1, vocab_dim,\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "        encoded.scatter_(2,\n",
        "                         last_token.view(1, 1, 1),\n",
        "                         1.0)\n",
        "\n",
        "        logits, hidden = model(encoded, hidden)\n",
        "\n",
        "        scaled_logits = logits[0, -1] / max(temp, 1e-6)\n",
        "        probabilities = torch.softmax(scaled_logits, dim=0)\n",
        "\n",
        "        sampled_id = torch.multinomial(probabilities, 1).item()\n",
        "        id_stream.append(sampled_id)\n",
        "\n",
        "    words = [id_to_token[i] for i in id_stream]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def run_onehot_rnn_pipeline():\n",
        "\n",
        "    corpus_text = read_poetry_file()\n",
        "\n",
        "    token_stream = (\n",
        "        [\"<bos>\"]\n",
        "        + normalize_and_split(corpus_text)\n",
        "        + [\"<eos>\"]\n",
        "    )\n",
        "\n",
        "    token_to_id, id_to_token = create_vocabulary(\n",
        "        token_stream,\n",
        "        min_occurrence=1\n",
        "    )\n",
        "\n",
        "    encoded_stream = encode_tokens(\n",
        "        token_stream,\n",
        "        token_to_id\n",
        "    )\n",
        "\n",
        "    window = 25\n",
        "    seq_inputs, seq_targets = build_sequence_pairs(\n",
        "        encoded_stream,\n",
        "        window\n",
        "    )\n",
        "\n",
        "    dataset = OneHotSequenceDataset(\n",
        "        seq_inputs,\n",
        "        seq_targets,\n",
        "        vocab_dim=len(token_to_id)\n",
        "    )\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    model = OneHotLanguageRNN(\n",
        "        vocab_dim=len(token_to_id),\n",
        "        hidden_dim=256\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=1e-3\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training One-Hot RNN on\", DEVICE)\n",
        "\n",
        "    timing_log = []\n",
        "    generated_samples = []\n",
        "\n",
        "    for epoch_idx in range(n_epochs):\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        loss_sum = 0.0\n",
        "        step_count = 0\n",
        "\n",
        "        for batch_x, batch_y in loader:\n",
        "\n",
        "            batch_x = batch_x.to(DEVICE)\n",
        "            batch_y = batch_y.to(DEVICE)\n",
        "\n",
        "            logits, _ = model(batch_x)\n",
        "\n",
        "            loss = criterion(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                batch_y.reshape(-1)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                model.parameters(), 1.0\n",
        "            )\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            step_count += 1\n",
        "\n",
        "        avg_loss = loss_sum / max(step_count, 1)\n",
        "        print(f\"Epoch {epoch_idx+1} | loss: {avg_loss:.4f}\")\n",
        "\n",
        "        sample_text = sample_sequence(\n",
        "            model,\n",
        "            token_to_id,\n",
        "            id_to_token,\n",
        "            seed=\"<bos>\",\n",
        "            steps=40,\n",
        "            temp=0.9\n",
        "        )\n",
        "\n",
        "        generated_samples.append(sample_text)\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        timing_log.append(float(end_time - start_time))\n",
        "\n",
        "        print(\"Sample:\", sample_text)\n",
        "        print(f\"Epoch time: {timing_log[-1]:.3f}s\")\n",
        "\n",
        "    print(\n",
        "        f\"Total training time (one-hot): \"\n",
        "        f\"{sum(timing_log):.2f}s\"\n",
        "    )\n",
        "\n",
        "    return timing_log, generated_samples[-1]\n",
        "\n",
        "\n",
        "onehot_epoch_times, onehot_last_output = run_onehot_rnn_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "208099e5",
        "outputId": "1f0ce6fd-afb7-4e75-bb48-f438aafe3811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Embedding RNN on mps\n",
            "Epoch 1 | loss: 5.0160\n",
            "Sample: <bos> dazzle , theology—but ' things them dim-descried ' s best , but i know , perhaps what is his brother at the wife with him on brotherly and invite , not one is more far hot made ; and am\n",
            "Epoch time: 14.533s\n",
            "Epoch 2 | loss: 2.4822\n",
            "Sample: <bos> sat and straw , over the long-leav ' d slave is lit with the iris sheen of the north , where the voice of enjoyment picks ' d , when thou art gone , my dear , i find him\n",
            "Epoch time: 14.467s\n",
            "Epoch 3 | loss: 1.1327\n",
            "Sample: <bos> sullen , one then stand on a heap ' d tale of nature ' s immortality , a venerable thing ? of hay the best , or through those drain ' d from your eyes , my burst rest in\n",
            "Epoch time: 14.395s\n",
            "Epoch 4 | loss: 0.6131\n",
            "Sample: <bos> text and come , with thee , thou : white peacocks , songs at eve , and antique maps of america . farewell at know that i love thee better . i could not die – here we go to\n",
            "Epoch time: 14.004s\n",
            "Epoch 5 | loss: 0.4282\n",
            "Sample: <bos> text your eyes over the sky . my voice is the wife so long white , his dungeon something like the shadows and his cattle , and the muse— nothing refuse . ' tis something , nay ' tis now\n",
            "Epoch time: 13.236s\n",
            "Epoch 6 | loss: 0.3513\n",
            "Sample: <bos> text your milky stream pale strippings of my life ! breast that presses against other breasts it shall be you ! you sweaty brooks and dews it shall be you ! winds whose soft-tickling genitals rub against me it shall\n",
            "Epoch time: 13.194s\n",
            "Epoch 7 | loss: 0.3112\n",
            "Sample: <bos> text and come to my soul , and you bitter hug of mortality , it is true , nor any more of my folks or of my voice loos ' d to the eddies of the wind , a few\n",
            "Epoch time: 13.176s\n",
            "Epoch 8 | loss: 0.2863\n",
            "Sample: <bos> text your hand on this spot i stand with my robust soul . \" \" o span of youth ! ever-push ' d elasticity ! o manhood , balanced , florid and full . my lovers suffocate me , crowding\n",
            "Epoch time: 13.190s\n",
            "Epoch 9 | loss: 0.2696\n",
            "Sample: <bos> text your tell nothing fall , it may be you transpire from the breasts of young men , it may be if i had known them i would astonish , neither bird nor tree , if mankind perished utterly ;\n",
            "Epoch time: 13.158s\n",
            "Epoch 10 | loss: 0.2576\n",
            "Sample: <bos> text ; out from the crowd steps the marksman , takes his position , levels his piece ; the groups of newly-come immigrants cover the wharf or levee , as the woolly-pates hoe in the sugar-field , the overseer views\n",
            "Epoch time: 13.118s\n",
            "Total training time (embedding): 136.47s\n"
          ]
        }
      ],
      "source": [
        "class IndexedSequenceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_ids, target_ids):\n",
        "        self.inputs = torch.tensor(input_ids, dtype=torch.long)\n",
        "        self.targets = torch.tensor(target_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.inputs.size(0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.inputs[index], self.targets[index]\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "class EmbeddingLanguageRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_dim,\n",
        "                 embedding_dim=128,\n",
        "                 hidden_dim=256):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(\n",
        "            vocab_dim, embedding_dim\n",
        "        )\n",
        "\n",
        "        self.recurrent = nn.RNN(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Linear(\n",
        "            hidden_dim, vocab_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids, hidden_state=None):\n",
        "\n",
        "        embedded = self.embedding_layer(token_ids)\n",
        "        rnn_out, next_state = self.recurrent(\n",
        "            embedded, hidden_state\n",
        "        )\n",
        "\n",
        "        logits = self.output_layer(rnn_out)\n",
        "\n",
        "        return logits, next_state\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_embedding_model(model,\n",
        "                           token_to_id,\n",
        "                           id_to_token,\n",
        "                           seed=\"<bos>\",\n",
        "                           steps=40,\n",
        "                           temp=1.0):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    seed_tokens = seed.split()\n",
        "    id_stream = [\n",
        "        token_to_id.get(tok, token_to_id[\"<unk>\"])\n",
        "        for tok in seed_tokens\n",
        "    ]\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(steps):\n",
        "\n",
        "        last_token = torch.tensor(\n",
        "            [[id_stream[-1]]],\n",
        "            dtype=torch.long,\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "        logits, hidden = model(last_token, hidden)\n",
        "\n",
        "        scaled = logits[0, -1] / max(temp, 1e-6)\n",
        "        probs = torch.softmax(scaled, dim=0)\n",
        "\n",
        "        next_id = torch.multinomial(probs, 1).item()\n",
        "        id_stream.append(next_id)\n",
        "\n",
        "    return \" \".join(\n",
        "        id_to_token[i] for i in id_stream\n",
        "    )\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def run_embedding_rnn_pipeline():\n",
        "\n",
        "    corpus_text = read_poetry_file()\n",
        "\n",
        "    token_stream = (\n",
        "        [\"<bos>\"]\n",
        "        + normalize_and_split(corpus_text)\n",
        "        + [\"<eos>\"]\n",
        "    )\n",
        "\n",
        "    token_to_id, id_to_token = create_vocabulary(\n",
        "        token_stream,\n",
        "        min_occurrence=1\n",
        "    )\n",
        "\n",
        "    encoded_stream = encode_tokens(\n",
        "        token_stream,\n",
        "        token_to_id\n",
        "    )\n",
        "\n",
        "    window = 25\n",
        "    seq_inputs, seq_targets = build_sequence_pairs(\n",
        "        encoded_stream,\n",
        "        window\n",
        "    )\n",
        "\n",
        "    dataset = IndexedSequenceDataset(\n",
        "        seq_inputs,\n",
        "        seq_targets\n",
        "    )\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    model = EmbeddingLanguageRNN(\n",
        "        vocab_dim=len(token_to_id),\n",
        "        embedding_dim=128,\n",
        "        hidden_dim=256\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=1e-3\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training Embedding RNN on\", DEVICE)\n",
        "\n",
        "    timing_log = []\n",
        "    generated_samples = []\n",
        "\n",
        "    for epoch_idx in range(n_epochs):\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        loss_sum = 0.0\n",
        "        step_count = 0\n",
        "\n",
        "        for batch_x, batch_y in loader:\n",
        "\n",
        "            batch_x = batch_x.to(DEVICE)\n",
        "            batch_y = batch_y.to(DEVICE)\n",
        "\n",
        "            logits, _ = model(batch_x)\n",
        "\n",
        "            loss = criterion(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                batch_y.reshape(-1)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                model.parameters(), 1.0\n",
        "            )\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            step_count += 1\n",
        "\n",
        "        avg_loss = loss_sum / max(step_count, 1)\n",
        "        print(f\"Epoch {epoch_idx+1} | loss: {avg_loss:.4f}\")\n",
        "\n",
        "        sample_text = sample_embedding_model(\n",
        "            model,\n",
        "            token_to_id,\n",
        "            id_to_token,\n",
        "            seed=\"<bos>\",\n",
        "            steps=40,\n",
        "            temp=0.9\n",
        "        )\n",
        "\n",
        "        generated_samples.append(sample_text)\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        timing_log.append(float(end_time - start_time))\n",
        "\n",
        "        print(\"Sample:\", sample_text)\n",
        "        print(f\"Epoch time: {timing_log[-1]:.3f}s\")\n",
        "\n",
        "    print(\n",
        "        f\"Total training time (embedding): \"\n",
        "        f\"{sum(timing_log):.2f}s\"\n",
        "    )\n",
        "\n",
        "    return timing_log, generated_samples[-1]\n",
        "\n",
        "\n",
        "embedding_epoch_times, embedding_last_output = run_embedding_rnn_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d369a0d",
        "outputId": "92ba5954-a50f-48f3-dc76-1bec2bad4b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training time (Scratch RNN): 848.27s\n",
            "Total training time (One-Hot RNN): 245.56s\n",
            "Total training time (Embedding RNN): 136.47s\n"
          ]
        }
      ],
      "source": [
        "scratch_total_runtime = sum(scratch_epoch_times)\n",
        "onehot_total_runtime = sum(onehot_epoch_times)\n",
        "embedding_total_runtime = sum(embedding_epoch_times)\n",
        "\n",
        "print(\n",
        "    f\"Total training time (Scratch RNN): \"\n",
        "    f\"{scratch_total_runtime:.2f}s\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Total training time (One-Hot RNN): \"\n",
        "    f\"{onehot_total_runtime:.2f}s\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Total training time (Embedding RNN): \"\n",
        "    f\"{embedding_total_runtime:.2f}s\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a49ed3c3",
        "outputId": "7d5da4a4-592c-459e-9936-377c69d992b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scratch RNN Metrics:\n",
            "{'token_count': 30, 'unique_tokens': 9, 'unique_ratio': 0.3, 'repeat_2gram_ratio': 0.5862068965517241, 'repeat_3gram_ratio': 0.4642857142857143, 'top_tokens': [('and', 7), ('our', 6), ('wild', 6), ('green', 5), ('of', 2), ('agony', 1), ('dear', 1), ('least', 1), ('they', 1)]}\n",
            "\n",
            "One-Hot RNN Metrics:\n",
            "{'token_count': 41, 'unique_tokens': 34, 'unique_ratio': 0.8292682926829268, 'repeat_2gram_ratio': 0.025, 'repeat_3gram_ratio': 0.0, 'top_tokens': [('the', 2), ('!', 2), ('shall', 2), (\"'\", 2), ('d', 2), ('yet', 2), ('and', 2), ('<bos>', 1), ('fords', 1), ('park', 1)]}\n",
            "\n",
            "Embedding RNN Metrics:\n",
            "{'token_count': 41, 'unique_tokens': 30, 'unique_ratio': 0.7317073170731707, 'repeat_2gram_ratio': 0.0, 'repeat_3gram_ratio': 0.0, 'top_tokens': [('the', 7), (',', 4), (';', 2), ('his', 2), ('<bos>', 1), ('text', 1), ('out', 1), ('from', 1), ('crowd', 1), ('steps', 1)]}\n"
          ]
        }
      ],
      "source": [
        "scratch_metrics_summary = analyze_text_statistics(\n",
        "    scratch_last_output\n",
        ")\n",
        "\n",
        "onehot_metrics_summary = analyze_text_statistics(\n",
        "    onehot_last_output\n",
        ")\n",
        "\n",
        "embedding_metrics_summary = analyze_text_statistics(\n",
        "    embedding_last_output\n",
        ")\n",
        "\n",
        "print(\"Scratch RNN Metrics:\")\n",
        "print(scratch_metrics_summary)\n",
        "\n",
        "print(\"\\nOne-Hot RNN Metrics:\")\n",
        "print(onehot_metrics_summary)\n",
        "\n",
        "print(\"\\nEmbedding RNN Metrics:\")\n",
        "print(embedding_metrics_summary)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}